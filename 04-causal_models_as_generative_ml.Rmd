---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path="fig/")
```

```{r, echo=FALSE}
library(reticulate)
use_virtualenv('venv', require=TRUE)
```

# Reasoning about DAGs

1. Has everyone enrolled in Piazza?  
2. Office hours on Friday evening
3. Scribes (publish first)


```{python}
import torch
import pyro
pyro.set_rng_seed(101)
```

## Recap: Causal models as generative models

Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models.

### 5.1.1 Ladder of causality

There are three levels of causal inference and we call it the ladder of causality.

* ### Associative(Seeing):
    Two variables are associative if observing one changes the probability of observing the other. Most of the machine learning models are good at finding an association between variables or features.

    For Example, What does a symptom tell me about a disease?
    
    One problem with associative inference is it does not have causal implications. In the above example, it might be that both the symptom and the disease both are caused by hidden variable(confounder) lifestyle.  

* ### Intervention(Doing):
    In intervention, we override the normal causal structure, forcing a variable to take a value it might not have taken if the system were left alone. 

    For Example, If I take aspirin, will my headache be cured?

    note that this distribution is difference than `` P(cured|aspirin)`` because there might be a confounder cause. Interventions can be performed on any causal Bayesian networks.

* ### Counterfactual(Imagining):
    Counterfactual causal inferences reason about hypothetical situations, things that could happen. For example, answering a question like was it the aspirin that stopped my headache. To perform such inference you need Structural causal models which can be implemented using PPLs.

### Some definitions and notation

* Joint probability distribution: $P_{\mathbb{X}}$
* Density $P_{\mathbb{X}=x} = \pi(x_1, ..., x_d)$
* Bivariate $P_{Z, Y}$, marginal $P_{Z}$, conditional $P_{Z|Y}$
* Generative model $\mathbb{M}$ is a machine learning model that "entails" joint distribution, either explicitly or implicitly
* We denote the joint probability distribution "entailed" by a generative model as $P_{\mathbb{X}}^{\mathbb{M}}$
* Directed acyclic graph DAG $\mathbb(G) = (V, E)$, where E is a set of directed edges.
* Parents in the DAG: Parents of $X_j$ in the DAG $\mathbb(G)$ is denoted $\text{pa}_j^{\mathbb{G}}$
* A Bayesian network is a generative model that entails a joint distribution that factorizes over a DAG.
* A causal generative model is a generative model of a causal mechanism.
* A causal Bayesian networks is a causal generative model that is simply a Bayesian network where the direction of edges in the DAG represent causality.
* Probabilistic programming:  Writing generative models as program.  Usually done with a framework that provides a DSL and abstractions for inference
* "Causal program": Let's call this a probabilistic program that   As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect.

### Difference between Bayesian networks and probabilistic programming

* Bayesian network(BN) is a DAG, where each edge represents a causal effect between two nodes. In BNs, the joint distribution is a product of all the factored conditional probability distributions(CPDs). BN frameworks generally provide a  small set of parametric conditional probability distributions. For example, bnlearn provides multinomial or ordinal variables for a discrete variable, Gaussian for continuous variable and some basic regressions.

* Probablistic Programming Langugage(PPPL) is way much expressive than Bayesian network and 
let you reprsent relations any way you like as long as you can represent them in code. 
  * Using PPLs you can develop none-parametric causal models like Dirichlet Process. To understand this in detail lets take a look at the Chinese restaurant process metaphor which is similar to k-means but without fixed value for k. The metaphor is as follows:
Imagine a Chinese restaurant in which customers enter. A new customer sits down at a table with a probability proportional to the number of customers already sitting there. Additionally, a customer opens a new table with some probability. Bayesian networks cannot represent such a dynamic process with its static DAG.
  * Most PPLs implement advanced distributions like Gamma, Wishart and Dirichlet distributions. Moreover, PPL also lets you create new distribution which cannot do using bnlearn.
  * PPLs contain control flow (if, for, while) and recursion. which helps in creating open world model with variables that are avaible in models based on some condition. For example 
      ```
      X = Bernoulli(p)
      if X == 1:
          Y = Gaussian(0, 1)
      ```
      Here, Y is available in model when X is 1. You can also created complex model like gaussian random walk where each step depends on the previous step.
      ```
      X = Poisson(Î»)
      Y = zeros(X)
      Y[0] = [Gaussian(0, 1)]
      for i in range(1, X):
          Y[i] = Gaussian(Y[i-1], 1))
      ```
* Generative models let you generate samples, but actual usefulness of graphical models come from its ability to perform inference.  The following image is a generative model. In model first you sample start position at the top from categorical distribution and the ball ends in one of the slots based on Gaussian distribution. 
![Image](fig/inference.png)
In this model, generating samples is a trivial forward task in which first you sample categorical distribution and then sample Gaussian distribution to get the final position of the ball. Whereas inference is a more challenging backward task. For example, one of the possible inference question for this model is if you know the final position of the ball then can we infer anything about starting position of the ball?

    Inference in Bayesian networks is easy because of its constraints on types of models you can develop using DAG. You can use any of probabilistic graphical model inference algorithms such as belief propagation, variable elimination.
    
    Because of control flows and support for advanced distributions, in PPLs inference is tougher and hence users require some kind of inference expertise. That being said, PPLs provide inference abstractions and cutting-edge inference algorithms so users don't have to work from scratch. Moreover, PPLs Include tensor-based frameworks like Tensorflow and PyTorch, allow you to build on data science intuition. For example, mini batching which lets you, process groups of training examples, simultaneously to take advantage of modern hardware like GPUs to scale your model to a large dataset.  
    
## Reasoning with DAGs

###  Intuition
* DAGs as a graphical language for reasoning about conditional independence
* Impossible to learn a language all at once, we'll focus on learning what it can do for us

### Reading DAGs as factorizations

#### Recap on core concepts

* Conditional probability
* Conditional independence
  * Notation: $U \perp_{P_{\mathbb{X}}} W|V$
  * Implications to factorization
  * Conditional independence in the joint changes the DAG

### Core graphical concepts

* A path in $\mathbb{G}$ is a sequence of edges between two vertices
  * todo: formal notation, see page 82 of Peters 2017
* Pearl's d-seperation -- Reading conditional independence from the DAG
  * todo: formal notation
  * So what?  We saw how conditional independence shapes the DAG.  This shows 
* V-structures / Colliders
  * moral v-structure
  * immoral v-structure and conditional independence
    * Sprinkler example
    
### Taking a step back -- what does conditional independence have to do with causality?

* correlation vs causation
* Latent variables and confounding
* That v-structure example was causal
* If you can't remember, use the algorithm (bnlearn, pgmpy)
* Reduces the problem to reasoning about the joint probability distribution to graph algorithms.
* Without a DAG, no d-separation.  Could there be a more general form of d-separation that could operate on a probabilistic program?

### Markov Property

* Markov blanket
  * probability definition
  * DAG definition (slides)
  * Implications to prediction
* Markov property (slides)
  * Global
  * Local
  * Markov factorization
* Markov equivalence 
  * Recap: The definition conditional probability is P(A|B) = P(A,B)/P(B) (blackboard)
    * This definition means you can factorize any joint into a product of conditionals.  For example P(A, B, C) =  P(A)P(B|A)P(C|A, B)
    * A product of conditionals can be represented as a DAG.  In this case with edges {A-> B, B -> C, A->C}.
    * But you can also factorize P(A, B, C) in to P(C)P(B|C)P(A|B,C), getting edges {C->B, B->A, C->A}.
    * So you have two different DAGs that are equivalent factorizations of the joint probability.  Call this equivalence Markov equivalence.
    * Generally, given a DAG, the set that includes that DAG and all the DAGs that are Markov equivalent to that DAG are called a Markov equivalence class.
  * PDAG is a compact representation of the equivalence class
    * When you have an equivalence class of some thing, trying to find some meaningful representation of that class without enumerating all of its members is a hard thing to do.
    * Usually the best you can do is look for some kind of isomorphism between two objects to test if they are equivalent, according to some definition of equivalence
    * The nice thing about the equivalence classes of DAGs is that all the DAGs will have the same "skeleton", meaning set of connections between nodes.
    * The difference is that some or all of those edges will have different directions in different DAGs.
    * This is where we get the PDAG as a compact representation of an equivalence class.  The PDAG has the same skeleton as all of the members of the equivalence class.
    * The undirected edges in the PDAG correspond to edges that vary in direction among members of the class.
    * A directed edge in the PDAG mean that  all members of the class have that edge oriented in that direction.
  * There are other graphical representations of joint probability distributions
    * Undirected graph -- doesn't admit causal reasoning
    * Ancestral graphs (slide) -- Doesnt directly map to a generative model

## Causality and DAGs
  * Assume no latent variables (very strong assumption)
  * Causation vs correlation -- only two options
  * A second look at PDAGs
  
  


